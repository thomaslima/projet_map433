%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsmath}
\newcommand{\mean}{\mathbb{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\def\longlongrightarrow{\relbar\joinrel\relbar\joinrel\relbar\joinrel\rightarrow}
\usepackage[svgnames]{xcolor} % Required to specify font color
\newcommand*{\plogo}{\fbox{$\mathcal{PL}$}} % Generic publisher logo
\usepackage{graphicx} % Required for box manipulation
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\newcommand*{\titleTH}{\begingroup % Create the command for including the title page in the document
\raggedleft % Right-align all text
\vspace*{\baselineskip} % Whitespace at the top of the page

{\Large Bruno Morgado Dal Molin \\ Thomas Ferreira de Lima}\\[0.167\textheight] % Author name

{\textcolor{Red}{\huge MAP 433 \\Introduction aux \\ Méthodes Statistiques}}\\[\baselineskip] % Main title which draws the focus of the reader

{\LARGE\bfseries Estimation de Variance \\par Bootstrap}\\[\baselineskip] % First part of the title, if it is unimportant consider making the font size smaller to accentuate the main title

{\Large \textit{Professeur Marc Hoffmann}}\par % Tagline or further description

\vfill % Whitespace between the title block and the publisher
\vspace{2in}
%{\large The Publisher \plogo}\par % Publisher and logo

\vspace*{3\baselineskip} % Whitespace at the bottom of the page
\endgroup}

%----------------------------------------------------------------------------------------
%	DOCUMENT
%----------------------------------------------------------------------------------------

\begin{document}

\pagestyle{empty} % Removes page numbers

\titleTH % This command includes the title page


\section {Introduction}
	On considère le problème de l'estimation d'un paramètre de translation $\displaystyle \vartheta\in\R$
à partir de l'observation de la réalisation de n variables aléatoires indépendantes et identiquement distribuées
$\displaystyle (X_1,...,X_n)$ de loi commune

$$ \mathbb{P}[X_1\in A]= \int_{\cal A} g(x-\vartheta)dx,$$
pour tout ensemble borélien $\displaystyle A $ de $\displaystyle \R$, et où $\displaystyle x \leadsto g(x)$ est une densité de   probabilité inconnue vérifiant :

$$ \int_{\cal \R} xg(x)dx =0 , v^2 = \int_{\cal \R} x^2g(x)dx < +\infty,$$

Un estimateur naturel de $\displaystyle \vartheta$ est la moyenne empirique $\displaystyle \bar X_n = \frac{1}{n} \sum_{i=1}^n X_i$. On étudie dans ce projet la précision
d'estimation de $\displaystyle \bar X_n$, en particulier lorsque n n'est pas très grand, via la méthode de Bootstrap, introduite par Efron en 1979.
%%%%%%%%%%%%%%%%%%%%%%
\section {Comportement asymptotique de la variance empirique}
%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
Pour appliquer le TCL il suffit de montrer que $\mean[\bar{X}_i] = \vartheta$ et que $\var[\bar{X}_i] = v^2 < +\infty $. Or,
$$\mean [X_i] = \int_{\R}xg(x-\vartheta)dx=\int_{\R}(x-\vartheta)g(x-\vartheta)dx+\int_{\R}\vartheta g(x-\vartheta)dx$$
$$=\int_{\R}y g(y)dy+\vartheta\int_{\R}g(x-\vartheta)dx= \vartheta$$

$$\var[X_i] = \mean[X_i^2] - \mean[X_i]^2 = \int_{\R}x^2g(x-\vartheta)dx - \vartheta^2 = $$
$$ = \int_{\R}(x-\vartheta)^2g(x-\vartheta)dx + \int_{\R}2\vartheta(x-\vartheta)g(x-\vartheta)dx + \int_{\R}\vartheta^2 g(x-\vartheta)dx - \vartheta^2 = $$

$$= \int_{\R}(x-\vartheta)^2g(x-\vartheta)dx = v^2 < + \infty$$

Le TCL nous permet donc d'affirmer que:

$$\sqrt{n}(\vartheta - \bar X_n) \overset{d}{\longrightarrow} \mathcal{N} (0, v^2) $$
ou encore $$\xi_n = \frac{\sqrt{n}}{v}(\bar X_n - \vartheta) \overset{d}{\longrightarrow} \mathcal{N} (0, 1) $$

Ainsi, $$\mathbb{P}[\vartheta \in I_{n,\alpha}(v)]= \mathbb{P}[\bar X_n-\frac{v}{\sqrt{n}}\Phi^{-1}(1-\alpha/2)\leq \bar X_n-\frac{v}{\sqrt{n}}\xi_n\leq\bar X_n+\frac{v}{\sqrt{n}}\Phi^{-1}(1-\alpha/2)]$$
$$=\mathbb{P}[-\Phi^{-1}(1-\alpha/2)\leq \xi_n \leq\Phi^{-1}(1-\alpha/2)]$$

Puisque $\xi_n \overset{d}{\longrightarrow} \mathcal{N} (0, 1)$, on a que $\displaystyle \lim_{n \to \infty}\mathbb{P}(\xi_n \leq x)=\Phi(x) = \int_{-\infty}^xe^{-u^2/2}\frac{du}{\sqrt{2\pi}}$. D'où
$$\lim_{n \to \infty}\mathbb{P}[-\Phi^{-1}(1-\alpha/2)\leq \xi_n \leq\Phi^{-1}(1-\alpha/2)]=$$
$$=\lim_{n \to \infty}\mathbb{P}[\xi_n \leq\Phi^{-1}(1-\alpha/2)]-\lim_{n \to \infty}\mathbb{P}[\xi_n \leq\Phi^{-1}(\alpha/2)]$$
$$\Longrightarrow\lim_{n \to \infty}\mathbb{P}[\vartheta \in I_{n,\alpha}(v)]=1-\alpha/2-\alpha/2=1-\alpha$$
%%%%%%%%%%%%%%%%%%%%%%

On observe que $\displaystyle\int_{x}^{+\infty}\frac{(t^4+2t^2-1)}{(t^2+1)^2}e^{-t^2/2}dt=\frac{x}{1+x^2}e^{-x^2/2}$ et, en utilisant l'inegalité donnée $\displaystyle\frac{x}{(1+x^2)} \geq \frac{1}{2x}$ pour $\displaystyle |x|\geq 1$, on obtient:

$$\int_{x}^{+\infty}\frac{(t^4+2t^2-1)}{(t^2+1)^2}e^{-t^2/2}dt=\frac{x}{1+x^2}e^{-x^2/2} \geq \frac{e^{-x^2/2}}{2x}$$

On note aussi que $\displaystyle \frac{(t^4+2t^2-1)}{(t^2+1)^2} < 1 \ \forall t$, d'où:
$$1-\Phi(x)=\int_x^{+\infty}\frac{e^{-t^2/2}}{\sqrt{2\pi}}dt > \frac{1}{\sqrt{2\pi}}\int_{x}^{+\infty}\frac{(t^4+2t^2-1)}{(t^2+1)^2}e^{-t^2/2}dt =$$

$$ = \frac{x}{1+x^2}\frac{e^{-x^2/2}}{\sqrt{2\pi}} \geq \frac{e^{-x^2/2}}{2x\sqrt{2\pi}}$$
On conclut que:
$$1-\Phi(x)>\frac{e^{-x^2/2}}{2x\sqrt{2\pi}},\ \mathrm{pour}\ |x| \geq 1$$

Une autre inegalité est aussi importante pour la suite: $\displaystyle 1-\Phi(x) \leq \frac{1}{2} e^{-x^2/2}$. Demonstration:

$$1-\Phi(x) = \int_x^{+\infty} \frac{e^{-t^2/2}}{\sqrt{2\pi}} dt = \int_0^{+\infty} \frac{e^{-(t+x)^2/2}}{\sqrt{2\pi}} dt =$$
$$= \int_0^{+\infty} \frac{e^{-(t^2+x^2+2tx)/2}}{\sqrt{2\pi}} dt \leq \frac{e^{-x^2/2}}{\sqrt{2\pi}} \int_0^{+\infty} e^{-t^2/2} dt = \frac{e^{-x^2/2}}{2} $$

On utilise ces résultats pour étudier la relation du niveau de risque $\displaystyle \alpha$ et le nombre d'observations $\displaystyle n$: \\

$ \left\{
\begin{array}{ll}
\displaystyle 2x\sqrt{2\pi}(1-\Phi(x))\geq e^{-x^2/2} \\ \\
\displaystyle 1-\Phi(x) = \alpha/2
\end{array}
\right.
\Longrightarrow
$
$ \left\{
\begin{array}{ll}
\displaystyle 2x\sqrt{2\pi}(\alpha/2)\geq e^{-{(\Phi^{-1}(1-\alpha/2))}^2/2} \\ \\
\displaystyle \alpha \leq e^{-x^2/2} \Rightarrow x \leq \sqrt{2\log\left(\frac{1}{\alpha}\right)}
\end{array}
\right.
$


Ce qui nous donne directement:

$$e^{-\frac{1}{2}\left[\Phi^{-1}\left(1-\frac{\alpha}{2}\right)\right]^2} \leq \alpha\sqrt{4\pi\log\left(\frac{1}{\alpha}\right)}$$
$$\Longrightarrow \Phi^{-1}\left(1-\frac{\alpha}{2}\right) \geq \sqrt{2} \log\left(\alpha\sqrt{4\pi\log\left(\frac{1}{\alpha}\right)}\right)^{-\frac{1}{2}} \underset{\alpha \rightarrow 0}{\longlongrightarrow} +\infty$$

Cette inegalité nous permet d'estimer l'échelle de la fonction $\Phi^{-1}\left(1-\frac{\alpha}{2}\right)$, et donc d'estimer combien il faut augmenter $n$ pour améliorer la précision $\alpha$, en préservant la même précision d'estimation.

%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
Dans cette question, on utilise la \textbf{proposition 3.7} du policopie du cours:
\vspace{0.2in}

Si la fonctionnelle $\displaystyle T(F)$ admet la répresentation
$$T(F)=h\left(\int_{\R}g(x)dF(dx)\right)$$
où $\displaystyle \int_{\R}|g(x)|dF(dx)<+\infty$ et $\displaystyle h: \R\rightarrow\R$continue, alors,
$$ T(\hat F_n)\overset{p.s.}{\longrightarrow}T(F)$$
\\
De cette façon, en ayant $\displaystyle V_n=\frac{1}{n}\sum_{i=1}^n(X_i -\bar X_ n)^2=\frac{1}{n}\sum_{i=1}^nX_i^2-\bar X_n^2$,
$$V_n=\frac{1}{n}\sum_{i=1}^nX_i^2-\bar X_n^2=\frac{1}{n}\sum_{i=1}^nX_i^2-\left(\frac{1}{n}\sum_{i=1}^nX_i\right)$$
Et en choisissant des fonctions régulières satisfaisant les hypothèses de la proposition,
$$=h_{1}\left(\frac{1}{n}\sum_{i=1}^n g_{1}(X_i)\right)-h_{2}\left(\frac{1}{n}\sum_{i=1}^n g_{2}(X_i)\right)\overset{d}{\longrightarrow}$$
$$=h_1\left(\int_{\R}g_1(x)d\mathbb{P}(dx)\right)-h_2\left(\int_{\R}g_2(x)d\mathbb{P}(dx)\right)$$
$$=\int_{\R}x^2d\mathbb{P}(x)-\left({\int_{\R}xd\mathbb{P}(x)}\right)^2=\int_{\R}(x-m(\mathbb{P}))^2d\mathbb{P}(x)=v^2$$
pour les fonctions continues suivantes,
$ \left\{
\begin{array}{ll}
\displaystyle h_1(x)=x \\ \\
\displaystyle g_1(x)=x^2
\end{array}
\right.
$et
$ \left\{
\begin{array}{ll}
\displaystyle h_2(x)=x^2 \\ \\
\displaystyle g_2(x)=x
\end{array}
\right.
$.
Alors on peut affirmer,
$$v_{n}^2\overset{p.s.}{\longrightarrow}v^2$$
En prenant le résultat de la question précédente, on aura les conditions nécessaires pour appliquer le lemme de Slutsky,
$$ \left\{
\begin{array}{ll}
\displaystyle \frac{v_{n}}{v}\overset{\mathbb{P}}{\longrightarrow} 1\\ \\
\displaystyle \sqrt{n}(\theta-\bar X_{n})\overset{d}{\longrightarrow}\mathcal{N}(0,v^2)
\end{array}
\right.
$$
Donc,
$$\frac{\sqrt{n}(\theta-\bar X_{n})v}{v_{n}}\overset{d}{\longrightarrow}\mathcal{N}(0,v^2)$$
finalement,
$$ \sqrt{n}(\theta-\bar X_{n})\overset{d}{\longrightarrow}\mathcal{N}(0,v_{n}^{2})$$
Pour avoir encore les résultats de la question précédente concernant le niveau de risque et intervalle de confiance, il faut se rappeler que $\displaystyle V_{n}^{1/2}$ est définie comme $\displaystyle v_{n}$ et se ramener au résultat ci-dessus.
\section{Le ré-échantillonage par Bootstrap}
\subsection{}
On utilise la théorie d'estimation empirique de la fonction répartition en définissant pour a et b des fonctions régulières,
$$T(\hat F_{M})=a\left(\frac{1}{M}\sum_{j=1}^{M}b(X_{j}^{*})\right)\overset{p.s.}{\longrightarrow}T(F)=a\left(\mean_{\mathbb{Q}_{n}} [b(X^{*})]\right)$$
alors, on a la convergence dans le cas régulier lorsque $\displaystyle M\to \infty$ en prenant,
\\

$ \left\{
\begin{array}{ll}
\displaystyle a(x)=x \\ \\
\displaystyle b(x)=h(x)
\end{array}
\right.
$
$\Longrightarrow \frac{1}{M}\sum_{j=1}^{M}h(X_{j}^{*})\overset{p.s.}{\longrightarrow}\mean_{\mathbb{Q}_n} [h(X^{*})]$
\vspace{0.5 cm}
\\
En analysant la vitesse de convergence dans le cas régulier ont voit que,
$$\sqrt{n}\left(\frac{1}{M}\sum_{j=1}^{M}h(X_{j}^{*})-\mean_{\mathbb{Q}_{n}} [h(X^{*})]\right)\overset{d}{\longrightarrow}\mathcal{N}(0,v(F))$$
pour $\displaystyle v(F)=a'(\mean[b(X^{*})])^2Var(b(X^{*}))=Var[h(X^{*})]$ \\ \\
D'où on conclut la première égalité $\displaystyle \frac{1}{M}\sum_{j=1}^{M}h(X_{j}^{*}) \approx \mean_{\mathbb{Q_{n}}} [h(X^{*})]$.
\\
Pour la deuxième, on se ramène à la définition des variables aléatoires $\displaystyle X^*$ de loi $\displaystyle \mathbb{P}(X^*=X)=1/n$ d'après son caractère uniforme par rapport à l'échantillon de X,
$$\mean_{\mathbb{Q_{n}}} [h(X^{*})]=\sum_{i=1}^{n}\mathbb{P}(X^*=X_i)h(X_i)=\sum_{i=1}^{n}\frac{1}{n}h(X_i)=\frac{1}{n}\sum_{i=1}^{n}h(X_i)$$
Alors,
$$\mean_{\mathbb{Q_{n}}} [h(X^{*})]=\frac{1}{n}\sum_{i=1}^{n}h(X_i)$$
Pour la troisième égalité, on utilise un processus analogue à celui de la première. On prend des fonctions régulières a et b identiques à celles prises pour la première partie et on fait,
$$T(\hat F_{M})=a\left(\frac{1}{M}\sum_{i=1}^{M}b(X_{i})\right)\overset{p.s.}{\longrightarrow}T(F)=a\left(\int_{\mathbb{R}} b(X)g(x-\theta)dx\right)$$
De cette façon, on aura,
$$ \frac{1}{M}\sum_{i=1}^{M}h(X_{i})\overset{p.s.}{\longrightarrow}\int_{\mathbb{R}} h(X)g(x-\theta)dx$$
Ainsi que,
$$\sqrt{n}\left(\frac{1}{M}\sum_{i=1}^{M}h(X_{i})-\int_{\mathbb{R}} h(X)g(x-\theta)dx\right)\overset{d}{\longrightarrow}\mathcal{N}(0,v(F))$$
pour $\displaystyle v(F)=a'(\mean[b(X^{*})])^2\cdot\var(b(X^{*}))=\var[h(X)]$
\vspace{0.5cm} \\
D'où, finalement, on conclut la dernière égalité,
\\ $\displaystyle \frac{1}{M}\sum_{i=1}^{M}h(X_{i}) \approx \int_{\mathbb{R}} h(X)g(x-\theta)dx$
\vspace{1 cm}
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\\
\\
\\ {\Huge Daqui pra frente é ladainha mas deixe aí que talvez precise}


Il nous est proposé l'approximation suivante:
$$\vartheta=\bar X_n + \frac{v}{\sqrt{n}}\xi_n$$
On cherche donc la fonction de vraisemblance associé à l'expérience produit,
$$\vartheta \in \Theta \leadsto \mathcal{L}_n(\vartheta,X_1,...,X_n )= \prod_{i=1}^nf(\vartheta,X_i) $$
En sachant la loi de $\displaystyle \xi_n$ et sa distribuition, on trouve,
$$\mathbb{P}(dx)=\frac{1}{\sqrt{2\pi v^2}}e^{-n(\vartheta-\bar x_n)/2}dx$$
Comme $\displaystyle \mu(dx)>> \mathbb{P}(dx)$, on peut faire $\displaystyle f(\vartheta)=\frac{1}{\sqrt{2\pi v^2}}e^{-n(\vartheta-\bar x_n)/2}$

Alors,
$$\mathcal{L}_n(\vartheta,X_1,...,X_n )=(2\pi v^2)^{-n/2}e^{-n/2\sum_{i=1}^n(\vartheta - \bar X_ n)}$$
$$\ell_n(\vartheta,X_1,...,X_n )=-\frac{n}{2}(2\pi v^2)-\frac{n^2}{2}(\vartheta-\bar X_n)^2$$
$$\frac{d\ell_n}{d\vartheta}(\vartheta,X_1,...,X_n )=0 \Rightarrow\hat \vartheta_n=\frac{1}{n}\sum_{i=1}^nX_i$$
Donc $\displaystyle \vartheta$ est bien le estimateur par la moyenne empirique.



En ayant la suite de variables aléatoires $\displaystyle (X_1,...,X_n) $ iid de même loi de distribuition $\displaystyle g(x)$ dont:
$$\mu = \mean[X_i] = 0 \Rightarrow \mean[\sum_{i=1}^n X_i ] = 0 $$
Encore pour la variance, on appelle

$$\displaystyle v^2 = \int_{\cal \R} x^2g(x)dx = \mean[X_i^2]$$
puis que
$$\displaystyle Var[X_i] = \mean[X_i^2]-\mean^2[X_i] $$
De cette façon,
$$v_{somme}^2=\mean[\sum_{i=1}^n X_i ]= nv^2$$


On se rappelle donc que $\displaystyle\vartheta = \bar X_n + \frac{v}{\sqrt{n}}\xi_n$ est valable selon les données précedentes et l'application direct du Théorème Central Limite.

Alors, pour $\displaystyle \alpha\in(0,1)$,
$$\mathbb{P}(|\bar X_n |<\varepsilon)>1-\alpha$$
$$\lim_{n \to \infty}\mathbb{P}(\xi_n \leq \varepsilon)=\Phi(x) = \int_{-\infty}^xe^{-u^2/2}\frac{du}{\sqrt{2\pi}}$$

\end{document}


