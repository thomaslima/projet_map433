%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{amsmath}
\newcommand{\mean}{\mathbb{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\def\longlongrightarrow{\relbar\joinrel\relbar\joinrel\relbar\joinrel\rightarrow}
\usepackage[svgnames]{xcolor} % Required to specify font color
\newcommand*{\plogo}{\fbox{$\mathcal{PL}$}} % Generic publisher logo
\usepackage{graphicx} % Required for box manipulation
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\newcommand*{\titleTH}{\begingroup % Create the command for including the title page in the document
\raggedleft % Right-align all text
\vspace*{\baselineskip} % Whitespace at the top of the page

{\Large Bruno Morgado Dal Molin \\ Thomas Ferreira de Lima}\\[0.167\textheight] % Author name

{\textcolor{Red}{\huge MAP 433 \\Introduction aux \\ Méthodes Statistiques}}\\[\baselineskip] % Main title which draws the focus of the reader

{\LARGE\bfseries Estimation de Variance \\par Bootstrap}\\[\baselineskip] % First part of the title, if it is unimportant consider making the font size smaller to accentuate the main title

{\Large \textit{Professeur Marc Hoffmann}}\par % Tagline or further description

\vfill % Whitespace between the title block and the publisher
\vspace{2in}
%{\large The Publisher \plogo}\par % Publisher and logo

\vspace*{3\baselineskip} % Whitespace at the bottom of the page
\endgroup}

%----------------------------------------------------------------------------------------
%	DOCUMENT
%----------------------------------------------------------------------------------------

\begin{document}

\pagestyle{empty} % Removes page numbers

\titleTH % This command includes the title page


\section {Introduction}
	On considère le problème de l'estimation d'un paramètre de translation $\displaystyle \vartheta\in\R$
à partir de l'observation de la réalisation de n variables aléatoires indépendantes et identiquement distribuées
$\displaystyle (X_1,...,X_n)$ de loi commune

$$ \mathbb{P}[X_1\in A]= \int_{\cal A} g(x-\vartheta)dx,$$
pour tout ensemble borélien $\displaystyle A $ de $\displaystyle \R$, et où $\displaystyle x \leadsto g(x)$ est une densité de   probabilité inconnue vérifiant :

$$ \int_{\cal \R} xg(x)dx =0 , v^2 = \int_{\cal \R} x^2g(x)dx < +\infty,$$

Un estimateur naturel de $\displaystyle \vartheta$ est la moyenne empirique $\displaystyle \bar X_n = \frac{1}{n} \sum_{i=1}^n X_i$. On étudie dans ce projet la précision
d'estimation de $\displaystyle \bar X_n$, en particulier lorsque n n'est pas très grand, via la méthode de Bootstrap, introduite par Efron en 1979.
%%%%%%%%%%%%%%%%%%%%%%
\section {Comportement asymptotique de la variance empirique}
%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
Pour appliquer le TCL il suffit de montrer que $\mean[\bar{X}_i] = \vartheta$ et que $\var[\bar{X}_i] = v^2 < +\infty $. Or,
$$\mean [X_i] = \int_{\R}xg(x-\vartheta)dx=\int_{\R}(x-\vartheta)g(x-\vartheta)dx+\int_{\R}\vartheta g(x-\vartheta)dx$$
$$=\int_{\R}y g(y)dy+\vartheta\int_{\R}g(x-\vartheta)dx= \vartheta$$

$$\var[X_i] = \mean[X_i^2] - \mean[X_i]^2 = \int_{\R}x^2g(x-\vartheta)dx - \vartheta^2 = $$
$$ = \int_{\R}(x-\vartheta)^2g(x-\vartheta)dx + \int_{\R}2\vartheta(x-\vartheta)g(x-\vartheta)dx + \int_{\R}\vartheta^2 g(x-\vartheta)dx - \vartheta^2 = $$

$$= \int_{\R}(x-\vartheta)^2g(x-\vartheta)dx = v^2 < + \infty$$

Le TCL nous permet donc d'affirmer que:

$$\sqrt{n}(\vartheta - \bar X_n) \overset{d}{\longrightarrow} \mathcal{N} (0, v^2) $$
ou encore $$\xi_n = \frac{\sqrt{n}}{v}(\bar X_n - \vartheta) \overset{d}{\longrightarrow} \mathcal{N} (0, 1) $$

Ainsi, $$\mathbb{P}[\vartheta \in I_{n,\alpha}(v)]= \mathbb{P}[\bar X_n-\frac{v}{\sqrt{n}}\Phi^{-1}(1-\alpha/2)\leq \bar X_n-\frac{v}{\sqrt{n}}\xi_n\leq\bar X_n+\frac{v}{\sqrt{n}}\Phi^{-1}(1-\alpha/2)]$$
$$=\mathbb{P}[-\Phi^{-1}(1-\alpha/2)\leq \xi_n \leq\Phi^{-1}(1-\alpha/2)]$$

Puisque $\xi_n \overset{d}{\longrightarrow} \mathcal{N} (0, 1)$, on a que $\displaystyle \lim_{n \to \infty}\mathbb{P}(\xi_n \leq x)=\Phi(x) = \int_{-\infty}^xe^{-u^2/2}\frac{du}{\sqrt{2\pi}}$. D'où
$$\lim_{n \to \infty}\mathbb{P}[-\Phi^{-1}(1-\alpha/2)\leq \xi_n \leq\Phi^{-1}(1-\alpha/2)]=$$
$$=\lim_{n \to \infty}\mathbb{P}[\xi_n \leq\Phi^{-1}(1-\alpha/2)]-\lim_{n \to \infty}\mathbb{P}[\xi_n \leq\Phi^{-1}(\alpha/2)]$$
$$\Longrightarrow\lim_{n \to \infty}\mathbb{P}[\vartheta \in I_{n,\alpha}(v)]=1-\alpha/2-\alpha/2=1-\alpha$$
%%%%%%%%%%%%%%%%%%%%%%

On observe que $\displaystyle\int_{x}^{+\infty}\frac{(t^4+2t^2-1)}{(t^2+1)^2}e^{-t^2/2}dt=\frac{x}{1+x^2}e^{-x^2/2}$ et, en utilisant l'inegalité donnée $\displaystyle\frac{x}{(1+x^2)} \geq \frac{1}{2x}$ pour $\displaystyle |x|\geq 1$, on obtient:

$$\int_{x}^{+\infty}\frac{(t^4+2t^2-1)}{(t^2+1)^2}e^{-t^2/2}dt=\frac{x}{1+x^2}e^{-x^2/2} \geq \frac{e^{-x^2/2}}{2x}$$

On note aussi que $\displaystyle \frac{(t^4+2t^2-1)}{(t^2+1)^2} < 1 \ \forall t$, d'où:
$$1-\Phi(x)=\int_x^{+\infty}\frac{e^{-t^2/2}}{\sqrt{2\pi}}dt > \frac{1}{\sqrt{2\pi}}\int_{x}^{+\infty}\frac{(t^4+2t^2-1)}{(t^2+1)^2}e^{-t^2/2}dt =$$

$$ = \frac{x}{1+x^2}\frac{e^{-x^2/2}}{\sqrt{2\pi}} \geq \frac{e^{-x^2/2}}{2x\sqrt{2\pi}}$$
On conclut que:
$$1-\Phi(x)>\frac{e^{-x^2/2}}{2x\sqrt{2\pi}},\ \mathrm{pour}\ |x| \geq 1$$

Une autre inegalité est aussi importante pour la suite: $\displaystyle 1-\Phi(x) \leq \frac{1}{2} e^{-x^2/2}$. Demonstration:

$$1-\Phi(x) = \int_x^{+\infty} \frac{e^{-t^2/2}}{\sqrt{2\pi}} dt = \int_0^{+\infty} \frac{e^{-(t+x)^2/2}}{\sqrt{2\pi}} dt =$$
$$= \int_0^{+\infty} \frac{e^{-(t^2+x^2+2tx)/2}}{\sqrt{2\pi}} dt \leq \frac{e^{-x^2/2}}{\sqrt{2\pi}} \int_0^{+\infty} e^{-t^2/2} dt = \frac{e^{-x^2/2}}{2} $$

On utilise ces résultats pour étudier la relation du niveau de risque $\displaystyle \alpha$ et le nombre d'observations $\displaystyle n$: \\

$ \left\{
\begin{array}{ll}
\displaystyle 2x\sqrt{2\pi}(1-\Phi(x))\geq e^{-x^2/2} \\ \\
\displaystyle 1-\Phi(x) = \alpha/2
\end{array}
\right.
\Longrightarrow
$
$ \left\{
\begin{array}{ll}
\displaystyle 2x\sqrt{2\pi}(\alpha/2)\geq e^{-{(\Phi^{-1}(1-\alpha/2))}^2/2} \\ \\
\displaystyle \alpha \leq e^{-x^2/2} \Rightarrow x \leq \sqrt{2\log\left(\frac{1}{\alpha}\right)}
\end{array}
\right.
$


Ce qui nous donne directement:

$$e^{-\frac{1}{2}\left[\Phi^{-1}\left(1-\frac{\alpha}{2}\right)\right]^2} \leq \alpha\sqrt{4\pi\log\left(\frac{1}{\alpha}\right)}$$
$$\Longrightarrow \Phi^{-1}\left(1-\frac{\alpha}{2}\right) \geq \sqrt{2} \log\left(\alpha\sqrt{4\pi\log\left(\frac{1}{\alpha}\right)}\right)^{-\frac{1}{2}} \underset{\alpha \rightarrow 0}{\longlongrightarrow} +\infty$$

Cette inegalité nous permet d'estimer l'échelle de la fonction $\Phi^{-1}\left(1-\frac{\alpha}{2}\right)$, et donc d'estimer combien il faut augmenter $n$ pour améliorer la précision $\alpha$, en préservant la même précision d'estimation.

%%%%%%%%%%%%%%%%%%%%%%
\subsection{}
Dans cette question, on utilise la \textbf{proposition 3.7} du policopie du cours:
\vspace{0.2in}

Si la fonctionnelle $\displaystyle T(F)$ admet la répresentation
$$T(F)=h\left(\int_{\R}g(x)dF(dx)\right)$$
où $\displaystyle \int_{\R}|g(x)|dF(dx)<+\infty$ et $\displaystyle h: \R\rightarrow\R$continue, alors,
$$ T(\hat F_n)\overset{p.s.}{\longrightarrow}T(F)$$
\\
De cette façon, en ayant $\displaystyle V_n=\frac{1}{n}\sum_{i=1}^n(X_i -\bar X_ n)^2=\frac{1}{n}\sum_{i=1}^nX_i^2-\bar X_n^2$,
$$V_n=\frac{1}{n}\sum_{i=1}^nX_i^2-\bar X_n^2=\frac{1}{n}\sum_{i=1}^nX_i^2-\left(\frac{1}{n}\sum_{i=1}^nX_i\right)$$
Et en choisissant des fonctions régulières satisfaisant les hypothèses de la proposition,
$$=h_{1}\left(\frac{1}{n}\sum_{i=1}^n g_{1}(X_i)\right)-h_{2}\left(\frac{1}{n}\sum_{i=1}^n g_{2}(X_i)\right)\overset{d}{\longrightarrow}$$
$$=h_1\left(\int_{\R}g_1(x)d\mathbb{P}(dx)\right)-h_2\left(\int_{\R}g_2(x)d\mathbb{P}(dx)\right)$$
$$=\int_{\R}x^2d\mathbb{P}(x)-\left({\int_{\R}xd\mathbb{P}(x)}\right)^2=\int_{\R}(x-m(\mathbb{P}))^2d\mathbb{P}(x)=v^2$$
pour les fonctions continues suivantes,
$ \left\{
\begin{array}{ll}
\displaystyle h_1(x)=x \\ \\
\displaystyle g_1(x)=x^2
\end{array}
\right.
$et
$ \left\{
\begin{array}{ll}
\displaystyle h_2(x)=x^2 \\ \\
\displaystyle g_2(x)=x
\end{array}
\right.
$.
Alors on peut affirmer,
$$v_{n}^2\overset{p.s.}{\longrightarrow}v^2$$
En prenant le résultat de la question précédente, on aura les conditions nécessaires pour appliquer le lemme de Slutsky,
$$ \left\{
\begin{array}{ll}
\displaystyle \frac{v_{n}}{v}\overset{\mathbb{P}}{\longrightarrow} 1\\ \\
\displaystyle \sqrt{n}(\theta-\bar X_{n})\overset{d}{\longrightarrow}\mathcal{N}(0,v^2)
\end{array}
\right.
$$
Donc,
$$\frac{\sqrt{n}(\theta-\bar X_{n})v}{v_{n}}\overset{d}{\longrightarrow}\mathcal{N}(0,v^2)$$
finalement,
$$ \sqrt{n}(\theta-\bar X_{n})\overset{d}{\longrightarrow}\mathcal{N}(0,v_{n}^{2})$$
Pour avoir encore les résultats de la question précédente concernant le niveau de risque et intervalle de confiance, il faut se rappeler que $\displaystyle V_{n}^{1/2}$ est définie comme $\displaystyle v_{n}$ et se ramener au résultat ci-dessus.
\section{Le ré-échantillonage par Bootstrap}
\subsection{}
On utilise la théorie d'estimation empirique de la fonction répartition en définissant pour a et b des fonctions régulières,
$$T(\hat F_{M})=a\left(\frac{1}{M}\sum_{j=1}^{M}b(X_{j}^{*})\right)\overset{p.s.}{\longrightarrow}T(F)=a\left(\mean_{\mathbb{Q}_{n}} [b(X^{*})]\right)$$
alors, on a la convergence dans le cas régulier lorsque $\displaystyle M\to \infty$ en prenant,
\\

$ \left\{
\begin{array}{ll}
\displaystyle a(x)=x \\ \\
\displaystyle b(x)=h(x)
\end{array}
\right.
$
$\Longrightarrow \frac{1}{M}\sum_{j=1}^{M}h(X_{j}^{*})\overset{p.s.}{\longrightarrow}\mean_{\mathbb{Q}_n} [h(X^{*})]$
\vspace{0.5 cm}
\\
En analysant la vitesse de convergence dans le cas régulier ont voit que,
$$\sqrt{n}\left(\frac{1}{M}\sum_{j=1}^{M}h(X_{j}^{*})-\mean_{\mathbb{Q}_{n}} [h(X^{*})]\right)\overset{d}{\longrightarrow}\mathcal{N}(0,v(F))$$
pour $\displaystyle v(F)=a'(\mean[b(X^{*})])^2Var(b(X^{*}))=Var[h(X^{*})]$ \\ \\
D'où on conclut la première égalité $\displaystyle \frac{1}{M}\sum_{j=1}^{M}h(X_{j}^{*}) \approx \mean_{\mathbb{Q_{n}}} [h(X^{*})]$.
\\
Pour la deuxième, on se ramène à la définition des variables aléatoires $\displaystyle X^*$ de loi $\displaystyle \mathbb{P}(X^*=X)=1/n$ d'après son caractère uniforme par rapport à l'échantillon de X,
$$\mean_{\mathbb{Q_{n}}} [h(X^{*})]=\sum_{i=1}^{n}\mathbb{P}(X^*=X_i)h(X_i)=\sum_{i=1}^{n}\frac{1}{n}h(X_i)=\frac{1}{n}\sum_{i=1}^{n}h(X_i)$$
Alors,
$$\mean_{\mathbb{Q_{n}}} [h(X^{*})]=\frac{1}{n}\sum_{i=1}^{n}h(X_i)$$
Pour la troisième égalité, on utilise un processus analogue à celui de la première. On prend des fonctions régulières a et b identiques à celles prises pour la première partie et on fait,
$$T(\hat F_{M})=a\left(\frac{1}{M}\sum_{i=1}^{M}b(X_{i})\right)\overset{p.s.}{\longrightarrow}T(F)=a\left(\int_{\mathbb{R}} b(X)g(x-\theta)dx\right)$$
De cette façon, on aura,
$$ \frac{1}{M}\sum_{i=1}^{M}h(X_{i})\overset{p.s.}{\longrightarrow}\int_{\mathbb{R}} h(X)g(x-\theta)dx$$
Ainsi que,
$$\sqrt{n}\left(\frac{1}{M}\sum_{i=1}^{M}h(X_{i})-\int_{\mathbb{R}} h(X)g(x-\theta)dx\right)\overset{d}{\longrightarrow}\mathcal{N}(0,v(F))$$
pour $\displaystyle v(F)=a'(\mean[b(X^{*})])^2\cdot\var(b(X^{*}))=\var[h(X)]$
\vspace{0.5cm} \\
D'où, finalement, on conclut la dernière égalité,
\\ $\displaystyle \frac{1}{M}\sum_{i=1}^{M}h(X_{i}) \approx \int_{\mathbb{R}} h(X)g(x-\theta)dx$
\vspace{1 cm}
\section{Bootstrap par percentile}
\subsection{}
Selon les hypothèses de la question précédente concernants l'independence des variable aléatoires, on aura clairement que l'échantillon $\displaystyle \theta^{*}_{n,1}, \hdots, \theta^{*}_{n,M}$  est $\displaystyle iid$. Ce qui permet d'utiliser le TCL en se rappelant des résultats de la question $\displaystyle 3.1$,
$$\sqrt{M}\left(\frac{1}{M}\sum_{j=1}^{M}\theta^{*}_{n,j}-\frac{1}{n}\sum_{i=1}^{n}X_i\right)\overset{d}{\longrightarrow}\mathcal{N}(0,Var[\theta^{*}_{n}])$$
lorsque $\displaystyle M\to \infty$, où $\displaystyle Var[\theta^{*}_{n}]=\frac{V_n}{n}$.\\
On a encore que la loi des grands nombres et l'utilisation de la méthode de "plug-in", comme fait pour la question 2.2, impliquent, 
$$V^{(boot)}_{n,M}\overset{p.s.}{\longrightarrow}\frac{V_{n}}{n}$$
lorsque $\displaystyle M\to\infty$ où $\displaystyle V^{(boot)}_{n,M}=\frac{1}{M}\sum_{j=1}^{M}(\bar X^{*}_{n,j})^2-\left(\frac{1}{M}\sum_{j=1}^{M}\bar X^{*}_{n,j}\right)^2$. \\
De cette façon, on utilise le lemme de Slutsky, pour $\displaystyle g(a,b)=\frac{a}{b^{1/2}}$,
$$\sqrt{\frac{M}{ V^{(boot)}_{n,M}}}\left(\frac{1}{M}\sum_{j=1}^{M}\theta^{*}_{n,j}-\frac{1}{n}\sum_{i=1}^{n}X_i\right)\overset{d}{\longrightarrow}\mathcal{N}(0,1)$$
lorsque $\displaystyle M\to\infty$. \\
Comme conséquence directe de la convergence en loi,
$$F_{n,M}\overset{M\to\infty}{\longrightarrow}\Phi_{\mathcal{N}(\bar X_n, V_n/n)}(x)$$
Ce qui justifie la construction précédente.
\subsection{Thomas}
\subsection{}
En utilisant la fonction monotone donnée, on aura l'échantillon $\displaystyle \rho(\theta^{*}_{n,1}), \hdots, \rho(\theta^{*}_{n,M})$ qui, par le même raisonnement de la question 4.1, engendre la fonction répartition $\displaystyle Q_{n,M}$ et sa convergence en loi ci-dessus,
$$Q_{n,M}\overset{M\to\infty}{\longrightarrow}\Phi_{\mathcal{N}(\rho(\theta),Var[\rho(\bar X_n)])}(x)$$
en se rappelant que $\displaystyle \rho(\bar X_n)$ est une variable gaussienne de moyenne $\displaystyle \rho(\theta)$. \\ \\
Cela nous permettra cette fois-ci d'analyser l'intervalle de précision de $\theta$ en liant l'échantillon original étudié aux $M$ échantillons de taille $n$ établis, manipulation qui n'était pas possible pour la question $4.1$, car la loi à laquelle $F_{n,m}$ convergeait n'était pas centrée sur $\theta$, mais si sur $\bar X_n$. \\ \\
Finalement, en utilisant le fait que la fonction $\displaystyle x\leadsto \rho(x)$ est monotone on peut l'applique à l'inégalité réprésenttant l'intervalle de précision, c'est-à-dire,
$$\mathbb{P}\left(\theta \in I^{(p-boot)}_{n,M,\alpha}}\right)=\mathbb{P}\left(F^{-1}_{n,M}\left(\alpha/2\right)\leq \theta \leq F^{-1}_{n,M}\left(1-\alpha/2\right)\right)$$
égal à,
$$\mathbb{P}\left(Q^{-1}_{n,M}\left(\alpha/2\right)\leq \rho(\theta) \leq Q^{-1}_{n,M}\left(1-\alpha/2\right)\right)=1-\frac{\alpha}{2}-\frac{\alpha}{2}=1-\alpha$$

%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%

\end{document}
